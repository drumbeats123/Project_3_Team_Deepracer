# Team DeepRacer
# Project 3


## Our Team
Team DeepRacer is composed of Dhruba Chakrabarti, Dipesh Pandya, and Alexis Wukich.


## The Problem

Initially, we started off Project 3 to explore how we could use the concepts we learned in class to create music, also known as MusicGen. We explored several pre-existing tools, and discovered that Meta had a pre-trained AI model, AudioGen, that performed text-to-audio generation. 


We then brainstormed ways we could feed the Music Generator prompts and inputs. We realized that we could get details about movies using The Movie Database and feed them into the Music Generator to create music inspired by the movie elements. The Movie Database also provides images from movies, from movie posters and stills. Using their API, we could also retrieve URLs for these associated images. Given that we could generate music, we elected to try and add images to the generated images, to produce a Miniature Movie Trailer. 

## The Process

### Step 1: Extracting Details from TMDB

The process begins with a [query using movie title](https://api.themoviedb.org/3/search/movie?api_key={TMDB_API_KEY}&query={movie_title}) which provides the “movie id” which we need to get the details, including movie id, genre, and overview. Using the movie id query we can return the [keywords](https://api.themoviedb.org/3/movie/{movie_id}/keywords?api_key={TMDB_API_KEY}) and [images from the movie](https://api.themoviedb.org/3/movie/{movie_id}/images?api_key={TMDB_API_KEY}).


To go a step further, we added an emotions analysis of the keywords and overview to provide a tone with which we could prompt the Music Generator. 

### Step 2: Using Movie Sentiment Analyzer to extract Detected Emotions
To get these emotions used a pre-trained model developed by Meta known as [roBERTa](https://ai.meta.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/), which was an expansion of Google’s [BERT](https://research.google/pubs/bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding/). Using the input text from the extracted keywords and overview, the model to returns a keyword emotion and an overview emotion, because they were often different. The model returns one of seven emotions: anger, disgust, fear, joy, neutral, sadness, and surprise. Our output was “Decoded Emotions,” or the specific emotion returned for both keywords and overivew.

### Step 3: Pass through Outputs from TMDB Details and Movie Sentiment Analyzer to the User to create the “Mini Soundtrack”

The user inputs their movie of choice, and the outputs are generated and passed through to the user for viewing using a Gradio Interface.

Simultaneously, these outputs are passed through the prompt function that will be used for generating music. Using the pre-trained MusicGen model from [AudioCraft](https://audiocraft.metademolab.com/musicgen.html) from Meta, a music clip is generated by feeding in the prompt. The sound clip is exported into MP4 format and then looped to make 90-second clip. To make it authentic, 5 seconds of fade in and fade out are added at the beginning and end of the clip, respectively.

### Step 4: Merge the “Mini Soundtrack” with the TMDB Images to Create the “Mini Movie Trailer”

The following components are pulled together to create a movie trailer
Exported extended Sound clip
Image URLs retrieved from TMDB.

These two are put together to create a MP4 movie that the user can view online or download to share with friends and family.

## Installation

** Helpful Hint: ** For this project we recommend running the file in Google Colab. It will help to have your API keys for The Movie Database (“TMDB”) and Gemini saved as Secrets in Colab. For more information on using Secrets, you can see this helpful [article](https://medium.com/@parthdasawant/how-to-use-secrets-in-google-colab-450c38e3ec75).

The following libraries and dependencies are required to run the project successfully:
!pip install transformers
!pip install torch
!pip install datasets
!pip install python-dotenv
!pip install langchain
!pip install langchain-google-genai
!pip install langchain_community
!pip install gradio
!pip install audiocraft 
!pip install pydub 
import audiocraft
import gradio as gr
import moviepy.editor as mpy
from moviepy.editor import *
from moviepy.video.tools.segmenting import findObjects
import os
import soundfile as sf 
import torch
import requests
import json
import pandas as pd
from audiocraft.models import MusicGen
from dotenv import load_dotenv
from langchain.chains import APIChain
from langchain_google_genai import ChatGoogleGenerativeAI
from pydub import AudioSegment
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification


## Repository Files
New Copy of Team_DeepRacers - Final Project (1).ipynb - Colab workbook with completed code
DeepRacer Final Presentation Compressed.pdf - PDF of final presentation slides



## Noteworthy Pain Points, Learning Experiences, and Opportunities for Further Research

### Opportunities 

If there were more time, this project presents a great opportunity for us to hone our prompt engineering skills to produce the best quality and most appropriate “Mini Soundtrack” using Music Gen. We would also love to explore how we expand the dimensions of the emotions even further to create more complexity for our prompt.

The Music Gen process as well as assembling the videos could be time consuming and could only run in Colab as opposed to VS Code.


## Resources Consulted and Credits
The following credit must be given for assistance with our project:


[AudioCraft] (https://audiocraft.metademolab.com/audiogen.html)
[Audio Craft Music Gen Git Hub](https://github.com/facebookresearch/audiocraft/blob/main/docs/MUSICGEN.md)
@article{copet2023simple,
    title={Simple and Controllable Music Generation},
    author={Jade Copet and Felix Kreuk and Itai Gat and Tal Remez and David Kant and Gabriel Synnaeve and Yossi Adi and Alexandre Défossez},
    year={2023},
    journal={arXiv preprint arXiv:2306.05284},
}

roBERTa documentation from [Hugging Face on Meta](https://huggingface.co/FacebookAI/roberta-large) and the [Extension](https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/)  by:
Jochen Hartmann, "Emotion English DistilRoBERTa-base". https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/, 2022.


TMDB [API Reference](https://developer.themoviedb.org/reference/intro/getting-started), [Guides](https://developer.themoviedb.org/docs/getting-started), and [Support Community](https://www.themoviedb.org/talk/category/5047958519c29526b50017d6).


Thank you to the following individuals for encouraging us as we tried to stretch the concepts we learned in class:

Julia Kim
Anthony Inthavong
